{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running in Google Colab:\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/TimeSeriesForecasting')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import Trainer, visualize_predictions\n",
    "from model import Model\n",
    "from torch.utils.data import  DataLoader\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import TimeSeriesDataset\n",
    "import numpy as np\n",
    "from utils import plot_training_history, compare_experiments, print_experiment_summary\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c334045",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/*.csv'\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-3\n",
    "LOOKBACK_STEPS = 20\n",
    "FORECAST_STEPS = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f857ba9",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = sorted(glob.glob(DATA_PATH))\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "data = df.iloc[:, 1:].copy()\n",
    "data = data.replace(-9999, np.nan).ffill().dropna()\n",
    "\n",
    "for col in data.columns:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3 * std) & (data[col] <= mean + 3 * std)]\n",
    "\n",
    "data = data.values\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "# pd.DataFrame(data_normalized).hist(bins=50, figsize=(20, 15)) # uncomment to visualize feature distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1e472",
   "metadata": {},
   "source": [
    "## Create Dataset and Initialize Model\n",
    "\n",
    "Change model configs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(data_normalized))\n",
    "train_data = TimeSeriesDataset(data_normalized[:train_size], lookback=LOOKBACK_STEPS, forecast=FORECAST_STEPS)\n",
    "test_data = TimeSeriesDataset(data_normalized[train_size:], lookback=LOOKBACK_STEPS, forecast=FORECAST_STEPS)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"Train data shape: {len(train_data)}\")\n",
    "print(f\"Test data shape: {len(test_data)}\")\n",
    "\n",
    "model = Model(\n",
    "    num_variates=data_normalized.shape[1],\n",
    "    lookback_steps=LOOKBACK_STEPS,\n",
    "    forecast_steps=FORECAST_STEPS,\n",
    "\n",
    "    d_model=128,  # Embedding dimension (same for both streams)\n",
    "\n",
    "    # Stream 1: iTransformer\n",
    "    n_heads_s1=4,\n",
    "    n_layers_s1=2,\n",
    "    d_ff_s1=128,\n",
    "    dropout_s1=0.3,\n",
    "\n",
    "    # Stream 2: Powerformer\n",
    "    patch_len=2,\n",
    "    stride=8,\n",
    "    n_heads_s2=4,\n",
    "    n_layers_s2=2,\n",
    "    d_ff_s2=128,\n",
    "    dropout_s2=0.3,\n",
    "    attn_decay_scale=0.25,\n",
    "\n",
    "    # Fusion\n",
    "    n_heads_fusion=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93277c",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3d342",
   "metadata": {},
   "source": [
    "Change training configs here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e99d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train(\n",
    "    num_epochs=2,                            # Number of training epochs\n",
    "    use_scheduler=True,                      # Enable ReduceLROnPlateau scheduler\n",
    "    scheduler_factor=0.5,                    # LR reduction factor (new_lr = lr * factor)\n",
    "    scheduler_patience=1,                    # Epochs to wait before reducing LR\n",
    "    save_dir='./experiments',                # Directory to save models and metrics\n",
    "    save_metrics=True,                       # Save training metrics as JSON\n",
    "    experiment_name='test_small_cpu',                  # Name for this experiment (used in filenames)\n",
    "    results_file='all_experiments.json',     # Shared file for all experiments (None for individual files)\n",
    "    append_results=True,                     # Append to results_file (False to overwrite)\n",
    "    plot_predictions=True,                   # Generate prediction plots at end of training\n",
    "    num_plot_samples=5,                      # Number of random samples to visualize\n",
    "    num_plot_variates=4                      # Number of variates to show per sample\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265b9c6",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c934784",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    device='cpu',\n",
    "    num_samples=10,\n",
    "    num_variates=10,\n",
    "    save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83970976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all experiments in file\n",
    "print_experiment_summary('all_experiments.json')\n",
    "\n",
    "# Print specific experiment\n",
    "print_experiment_summary('all_experiments.json', experiment_name='test_small_cpu')\n",
    "\n",
    "# Plot specific experiment\n",
    "plot_training_history('all_experiments.json', experiment_name='test_small_cpu')\n",
    "\n",
    "# Plot most recent experiment (no name provided)\n",
    "#plot_training_history('all_experiments.json')\n",
    "\n",
    "# Compare specific experiments\n",
    "#compare_experiments('all_experiments.json', experiment_names=['baseline_v1', 'with_skip']) # no exeriment_name -> all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bafe74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
