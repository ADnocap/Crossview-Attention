{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running in Google Colab:\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/TimeSeriesForecasting')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Model\n",
    "from train import Trainer, visualize_predictions\n",
    "from utils import TimeSeriesDataset, plot_training_history, compare_experiments, print_experiment_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c334045",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/*.csv'\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-3\n",
    "LOOKBACK_STEPS = 336\n",
    "FORECAST_STEPS = 96\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f857ba9",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = sorted(glob.glob(DATA_PATH))\n",
    "dfs = [pd.read_csv(file, encoding='latin1') for file in csv_files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "data = df.iloc[:, 1:].copy()\n",
    "data = data.replace(-9999, np.nan).ffill().dropna()\n",
    "\n",
    "for col in data.columns:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3 * std) & (data[col] <= mean + 3 * std)]\n",
    "\n",
    "data = data.values\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "# pd.DataFrame(data_normalized).hist(bins=50, figsize=(20, 15)) # uncomment to visualize feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019df8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce1e472",
   "metadata": {},
   "source": [
    "## Create Dataset and Initialize Model\n",
    "\n",
    "Change model configs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(data_normalized))\n",
    "train_data = TimeSeriesDataset(data_normalized[:train_size], lookback=LOOKBACK_STEPS, forecast=FORECAST_STEPS)\n",
    "test_data = TimeSeriesDataset(data_normalized[train_size:], lookback=LOOKBACK_STEPS, forecast=FORECAST_STEPS)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"Train data shape: {len(train_data)}\")\n",
    "print(f\"Test data shape: {len(test_data)}\")\n",
    "\n",
    "model = Model(\n",
    "    num_variates=data_normalized.shape[1],\n",
    "    lookback_steps=LOOKBACK_STEPS,\n",
    "    forecast_steps=FORECAST_STEPS,\n",
    "\n",
    "    d_model=128,  # Embedding dimension (same for both streams)\n",
    "\n",
    "    # Stream 1: iTransformer\n",
    "    n_heads_s1=4,\n",
    "    n_layers_s1=2,\n",
    "    d_ff_s1=128,\n",
    "    dropout_s1=0.3,\n",
    "\n",
    "    # Stream 2: Powerformer\n",
    "    patch_len=2,\n",
    "    stride=8,\n",
    "    n_heads_s2=4,\n",
    "    n_layers_s2=2,\n",
    "    d_ff_s2=128,\n",
    "    dropout_s2=0.3,\n",
    "    attn_decay_scale=0.5,\n",
    "\n",
    "    # Fusion\n",
    "    n_heads_fusion=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93277c",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3d342",
   "metadata": {},
   "source": [
    "Change training configs here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e99d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train(\n",
    "    num_epochs=2,                            # Number of training epochs\n",
    "    use_scheduler=True,                      # Enable ReduceLROnPlateau scheduler\n",
    "    scheduler_factor=0.5,                    # LR reduction factor (new_lr = lr * factor)\n",
    "    scheduler_patience=1,                    # Epochs to wait before reducing LR\n",
    "    save_dir='./experiments',                # Directory to save models and metrics\n",
    "    save_metrics=True,                       # Save training metrics as JSON\n",
    "    experiment_name='small_cpu',                  # Name for this experiment (used in filenames)\n",
    "    results_file='all_experiments.json',     # Shared file for all experiments (None for individual files)\n",
    "    append_results=True,                     # Append to results_file (False to overwrite)\n",
    "    plot_predictions=True,                   # Generate prediction plots at end of training\n",
    "    num_plot_samples=5,                      # Number of random samples to visualize\n",
    "    num_plot_variates=4                      # Number of variates to show per sample\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265b9c6",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62eabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history('all_experiments.json', experiment_name='small_cpu', experiment_dir='./experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c934784",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(\n",
    "    model=model,\n",
    "    data_loader=train_loader,\n",
    "    device='cpu',\n",
    "    num_samples=10,\n",
    "    num_variates=10,\n",
    "    save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83970976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all experiments in file\n",
    "print_experiment_summary('all_experiments.json')\n",
    "\n",
    "# Print specific experiment\n",
    "print_experiment_summary('all_experiments.json', experiment_name='test_small_cpu')\n",
    "\n",
    "# Plot specific experiment\n",
    "plot_training_history('all_experiments.json', experiment_name='test_small_cpu')\n",
    "\n",
    "# Compare specific experiments\n",
    "compare_experiments('all_experiments.json', experiment_names=['baseline_v1', 'with_skip']) # no exeriment_name -> all experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c2319",
   "metadata": {},
   "source": [
    "## Hyper-param search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bafe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_grid = {\n",
    "    # Model architecture\n",
    "    'd_model': [64, 128, 256],                    # Embedding dimension\n",
    "    'n_layers': [2, 3],                           # Depth (same for both streams)\n",
    "    'dropout': [0.3],                   # Regularization\n",
    "    \n",
    "    # Powerformer-specific\n",
    "    'patch_len': [4, 8, 16],                      # Temporal patch size\n",
    "    'stride': [4, 8],                             # Patch stride (stride=patch_len/2 is common)\n",
    "    'attn_decay_scale': [0.1, 0.25, 0.5],        # Power-law decay (alpha)\n",
    "    \n",
    "    # Training\n",
    "    'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "    'weight_decay': [1e-4, 1e-3],\n",
    "}\n",
    "\n",
    "MAX_TRIALS = 20  # Limit number of trials (random sample if > total combinations)\n",
    "NUM_EPOCHS = 10  # Epochs per trial (keep low for speed)\n",
    "RESULTS_FILE = 'hyperparam_search.json'\n",
    "\n",
    "# Generate parameter combinations\n",
    "all_combinations = list(itertools.product(*hyperparam_grid.values()))\n",
    "param_names = list(hyperparam_grid.keys())\n",
    "\n",
    "total_combinations = 1\n",
    "for values in hyperparam_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"Total hyperparameter combinations: {total_combinations}\")\n",
    "selected_combinations = all_combinations\n",
    "print(f\"Testing all {len(all_combinations)} combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622431fa",
   "metadata": {},
   "source": [
    "Start search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44cc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_idx, params in enumerate(selected_combinations, 1):\n",
    "    # Convert to dict\n",
    "    config = dict(zip(param_names, params))\n",
    "    \n",
    "    print(f\"TRIAL {trial_idx}/{len(selected_combinations)}\")\n",
    "    print(f\"Config: {config}\")\n",
    "    \n",
    "    # Derived hyperparameters (following best practices)\n",
    "    n_heads = 4 if config['d_model'] <= 128 else 8  # Must divide d_model\n",
    "    d_ff = config['d_model'] * 4  # Standard: 4x d_model\n",
    "    \n",
    "    try:\n",
    "        # Initialize model with current hyperparameters\n",
    "        model = Model(\n",
    "            num_variates=data_normalized.shape[1],\n",
    "            lookback_steps=LOOKBACK_STEPS,\n",
    "            forecast_steps=FORECAST_STEPS,\n",
    "            skip_connections=True,  # Keep skip connections on\n",
    "            \n",
    "            d_model=config['d_model'],\n",
    "            \n",
    "            # Stream 1: iTransformer\n",
    "            n_heads_s1=n_heads,\n",
    "            n_layers_s1=config['n_layers'],\n",
    "            d_ff_s1=d_ff,\n",
    "            dropout_s1=config['dropout'],\n",
    "            \n",
    "            # Stream 2: Powerformer\n",
    "            patch_len=config['patch_len'],\n",
    "            stride=config['stride'],\n",
    "            n_heads_s2=n_heads,\n",
    "            n_layers_s2=config['n_layers'],\n",
    "            d_ff_s2=d_ff,\n",
    "            dropout_s2=config['dropout'],\n",
    "            attn_decay_scale=config['attn_decay_scale'],\n",
    "            \n",
    "            # Fusion\n",
    "            n_heads_fusion=n_heads // 2,  # Half of stream heads\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            learning_rate=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        history = trainer.train(\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            use_scheduler=True,\n",
    "            scheduler_factor=0.5,\n",
    "            scheduler_patience=2,\n",
    "            save_dir='./experiments',\n",
    "            save_metrics=True,\n",
    "            experiment_name=f\"trial_{trial_idx}\",\n",
    "            results_file=RESULTS_FILE,\n",
    "            append_results=True,\n",
    "            plot_predictions=False,  # Skip plots during search\n",
    "        )\n",
    "        \n",
    "        # Print trial summary\n",
    "        final_mse = history['test_losses'][-1]\n",
    "        final_mae = history['test_maes'][-1]\n",
    "        best_mse = min(history['test_losses'])\n",
    "        print(f\"  Final MSE: {final_mse:.6f} | Best MSE: {best_mse:.6f} | Final MAE: {final_mae:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"{'-'*80}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Hyperparameter search completed!\")\n",
    "print(f\"Results saved to: {RESULTS_FILE}\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
