# crossview-attention
A research framework merging inverted variate-token Transformers with locality-biased attention masks. Examines how contrasting encoding layouts and causal decay interact in time-series forecasting tasks.
